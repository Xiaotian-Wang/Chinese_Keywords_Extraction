# Chinese_Keywords_Extraction
<<<<<<< HEAD
=======


In this project, we use Roberta model pretrained on Chinese corpora, and the resource from these two works:

@inproceedings{cui-etal-2020-revisiting,
  title = "Revisiting Pre-Trained Models for {C}hinese Natural Language Processing",
  author = "Cui, Yiming  and
    Che, Wanxiang  and
    Liu, Ting  and
    Qin, Bing  and
    Wang, Shijin  and
    Hu, Guoping",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2020.findings-emnlp.58",
  pages = "657--668",
}

and

@article{chinese-bert-wwm,
title={Pre-Training with Whole Word Masking for Chinese BERT},
author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},
journal={arXiv preprint arXiv:1906.08101},
year={2019}
}
>>>>>>> master
